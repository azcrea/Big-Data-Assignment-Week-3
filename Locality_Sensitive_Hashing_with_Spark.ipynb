{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Install Spark"
      ],
      "metadata": {
        "id": "-eTiJm_3VGgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBd3xH5gVIVS",
        "outputId": "a3e7a7b9-ca3c-4e8a-f6ff-e62e74c4ce3e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=ba9479c212966bcfd4b28e2c2d52a975253d5ba3167855d8f3036e1cad895dbf\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Initialize Apache Spark Context"
      ],
      "metadata": {
        "id": "aMUXQ8-VVLEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Apache Spark SQL\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark Session/Context\n",
        "# We are using local machine with all the CPU cores [*]\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Hello Pyspark\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Check spark session\n",
        "print(spark)"
      ],
      "metadata": {
        "id": "Kx81Vo_iVN1l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Downloading Dataset"
      ],
      "metadata": {
        "id": "WRZsGCp6VRKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle/\n",
        "!touch ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKnpLk7tVgs-",
        "outputId": "ca080316-8ba1-4b55-a3b9-1da32afa955d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.15)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.5.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.16)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_token = {\"username\":\"husniridhartazzikry\",\"key\":\"d7cd05ddab3da1e874cc4bfd1778f664\"}"
      ],
      "metadata": {
        "id": "xij9geQ_VnLv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "F1ESiRLKVxJJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download from https://www.kaggle.com/datasets/urbanbricks/wikipedia-promotional-articles\n",
        "\n",
        "!kaggle datasets download -d urbanbricks/wikipedia-promotional-articles"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcLkmiCzV1iH",
        "outputId": "60224ccd-9b89-4ec3-d4f9-732b9ef77aa0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading wikipedia-promotional-articles.zip to /content\n",
            " 96% 193M/201M [00:02<00:00, 81.4MB/s]\n",
            "100% 201M/201M [00:02<00:00, 71.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Unzip the downloaded files"
      ],
      "metadata": {
        "id": "RiiWO0xoV6rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip wikipedia-promotional-articles.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHhJY9YZV6FL",
        "outputId": "5864b318-51a6-4a5a-cda5-ab38d8eae3dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  wikipedia-promotional-articles.zip\n",
            "  inflating: good.csv                \n",
            "  inflating: promotional.csv         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LSH task always consists of three steps:\n",
        "\n",
        "\n",
        "1. Converting original data into vectors\n",
        "2. Calculate the hash using MinHash algorithm\n",
        "3. Searching the similar pairs using k-Nearest Neighbor, or join algorithm."
      ],
      "metadata": {
        "id": "4Dszdo7HVZY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read Dataset"
      ],
      "metadata": {
        "id": "y-s1LlpPWAAl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2YdUqUWVAnk",
        "outputId": "bf584811-402f-479d-e942-9d04799b0d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- advert: string (nullable = true)\n",
            " |-- coi: string (nullable = true)\n",
            " |-- fanpov: string (nullable = true)\n",
            " |-- pr: string (nullable = true)\n",
            " |-- resume: string (nullable = true)\n",
            " |-- url: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read CSV\n",
        "df = spark.read.option(\"header\", True).csv(\"/content/promotional.csv\")\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add an ID for the dataset\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "newsDF = df.withColumn(\"id\", monotonically_increasing_id())\n",
        "newsDF.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUWXCImJWCGA",
        "outputId": "84e966c9-645c-4a49-bdd7-587f9cb29863"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+---+------+---+------+--------------------+---+\n",
            "|                text|advert|coi|fanpov| pr|resume|                 url| id|\n",
            "+--------------------+------+---+------+---+------+--------------------+---+\n",
            "|1 Litre no Namida...|     0|  0|     1|  0|     0|https://en.wikipe...|  0|\n",
            "|1DayLater was fre...|     1|  1|     0|  0|     0|https://en.wikipe...|  1|\n",
            "|1E is a privately...|     1|  0|     0|  0|     0|https://en.wikipe...|  2|\n",
            "|1Malaysia pronoun...|     1|  0|     0|  0|     0|https://en.wikipe...|  3|\n",
            "|The Jerusalem Bie...|     1|  0|     0|  0|     0|https://en.wikipe...|  4|\n",
            "|1st Round Enterpr...|     0|  0|     0|  1|     0|https://en.wikipe...|  5|\n",
            "|2ergo is a provid...|     1|  0|     0|  0|     0|https://en.wikipe...|  6|\n",
            "|2N Telekomunikace...|     1|  0|     0|  0|     0|https://en.wikipe...|  7|\n",
            "|A 3D printing mar...|     1|  0|     0|  0|     0|https://en.wikipe...|  8|\n",
            "|3DR is an America...|     1|  1|     0|  0|     0|https://en.wikipe...|  9|\n",
            "|3D Systems, headq...|     1|  0|     0|  0|     0|https://en.wikipe...| 10|\n",
            "|3Delight, is 3D c...|     0|  0|     0|  0|     1|https://en.wikipe...| 11|\n",
            "|3DVIA is a brand ...|     1|  1|     0|  0|     0|https://en.wikipe...| 12|\n",
            "|3i Infotech Ltd e...|     1|  0|     0|  0|     0|https://en.wikipe...| 13|\n",
            "|3logy is the Pino...|     1|  0|     0|  0|     0|https://en.wikipe...| 14|\n",
            "|The 4 Hour Chef T...|     1|  0|     0|  1|     0|https://en.wikipe...| 15|\n",
            "|4Children was a c...|     1|  1|     0|  0|     0|https://en.wikipe...| 16|\n",
            "|If this article d...|     1|  0|     0|  0|     0|https://en.wikipe...| 17|\n",
            "|4G Americas is a ...|     0|  0|     0|  1|     0|https://en.wikipe...| 18|\n",
            "|4MMM identified o...|     1|  0|     0|  0|     0|https://en.wikipe...| 19|\n",
            "+--------------------+------+---+------+---+------+--------------------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total rows\n",
        "newsDF.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1VHigNNWLkT",
        "outputId": "8f229346-b0fd-4732-bfb9-e7045250e65c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23837"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing Tokenizer"
      ],
      "metadata": {
        "id": "4HNBjPRWWOvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the tokenizer\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "#Tokenized the words\n",
        "wordsDF = tokenizer.transform(newsDF)\n",
        "\n",
        "wordsDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqrO_CCIWNm0",
        "outputId": "2a489bdc-0bc1-4e44-89a6-58c8657e5628"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+\n",
            "|                text|advert|coi|fanpov| pr|resume|                 url| id|               words|\n",
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+\n",
            "|1 Litre no Namida...|     0|  0|     1|  0|     0|https://en.wikipe...|  0|[1, litre, no, na...|\n",
            "|1DayLater was fre...|     1|  1|     0|  0|     0|https://en.wikipe...|  1|[1daylater, was, ...|\n",
            "|1E is a privately...|     1|  0|     0|  0|     0|https://en.wikipe...|  2|[1e, is, a, priva...|\n",
            "|1Malaysia pronoun...|     1|  0|     0|  0|     0|https://en.wikipe...|  3|[1malaysia, prono...|\n",
            "|The Jerusalem Bie...|     1|  0|     0|  0|     0|https://en.wikipe...|  4|[the, jerusalem, ...|\n",
            "|1st Round Enterpr...|     0|  0|     0|  1|     0|https://en.wikipe...|  5|[1st, round, ente...|\n",
            "|2ergo is a provid...|     1|  0|     0|  0|     0|https://en.wikipe...|  6|[2ergo, is, a, pr...|\n",
            "|2N Telekomunikace...|     1|  0|     0|  0|     0|https://en.wikipe...|  7|[2n, telekomunika...|\n",
            "|A 3D printing mar...|     1|  0|     0|  0|     0|https://en.wikipe...|  8|[a, 3d, printing,...|\n",
            "|3DR is an America...|     1|  1|     0|  0|     0|https://en.wikipe...|  9|[3dr, is, an, ame...|\n",
            "|3D Systems, headq...|     1|  0|     0|  0|     0|https://en.wikipe...| 10|[3d, systems,, he...|\n",
            "|3Delight, is 3D c...|     0|  0|     0|  0|     1|https://en.wikipe...| 11|[3delight,, is, 3...|\n",
            "|3DVIA is a brand ...|     1|  1|     0|  0|     0|https://en.wikipe...| 12|[3dvia, is, a, br...|\n",
            "|3i Infotech Ltd e...|     1|  0|     0|  0|     0|https://en.wikipe...| 13|[3i, infotech, lt...|\n",
            "|3logy is the Pino...|     1|  0|     0|  0|     0|https://en.wikipe...| 14|[3logy, is, the, ...|\n",
            "|The 4 Hour Chef T...|     1|  0|     0|  1|     0|https://en.wikipe...| 15|[the, 4, hour, ch...|\n",
            "|4Children was a c...|     1|  1|     0|  0|     0|https://en.wikipe...| 16|[4children, was, ...|\n",
            "|If this article d...|     1|  0|     0|  0|     0|https://en.wikipe...| 17|[if, this, articl...|\n",
            "|4G Americas is a ...|     0|  0|     0|  1|     0|https://en.wikipe...| 18|[4g, americas, is...|\n",
            "|4MMM identified o...|     1|  0|     0|  0|     0|https://en.wikipe...| 19|[4mmm, identified...|\n",
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the dataset\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "vocabSize=1000\n",
        "\n",
        "# Train the CountVectorizer Model using our data\n",
        "cvModel = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=vocabSize, minDF=10).fit(wordsDF)\n",
        "\n",
        "# Transform our data into vector\n",
        "vectorizedDF = cvModel.transform(wordsDF)\n",
        "vectorizedDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD8Zp1kkWYlB",
        "outputId": "469d30ec-674c-4b01-e0cb-ee7112bd8c0d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+--------------------+\n",
            "|                text|advert|coi|fanpov| pr|resume|                 url| id|               words|            features|\n",
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+--------------------+\n",
            "|1 Litre no Namida...|     0|  0|     1|  0|     0|https://en.wikipe...|  0|[1, litre, no, na...|(1000,[0,1,2,3,4,...|\n",
            "|1DayLater was fre...|     1|  1|     0|  0|     0|https://en.wikipe...|  1|[1daylater, was, ...|(1000,[0,1,2,3,4,...|\n",
            "|1E is a privately...|     1|  0|     0|  0|     0|https://en.wikipe...|  2|[1e, is, a, priva...|(1000,[0,1,2,3,4,...|\n",
            "|1Malaysia pronoun...|     1|  0|     0|  0|     0|https://en.wikipe...|  3|[1malaysia, prono...|(1000,[0,1,2,3,4,...|\n",
            "|The Jerusalem Bie...|     1|  0|     0|  0|     0|https://en.wikipe...|  4|[the, jerusalem, ...|(1000,[0,1,2,3,4,...|\n",
            "|1st Round Enterpr...|     0|  0|     0|  1|     0|https://en.wikipe...|  5|[1st, round, ente...|(1000,[0,1,2,3,4,...|\n",
            "|2ergo is a provid...|     1|  0|     0|  0|     0|https://en.wikipe...|  6|[2ergo, is, a, pr...|(1000,[0,1,2,3,4,...|\n",
            "|2N Telekomunikace...|     1|  0|     0|  0|     0|https://en.wikipe...|  7|[2n, telekomunika...|(1000,[0,1,2,3,4,...|\n",
            "|A 3D printing mar...|     1|  0|     0|  0|     0|https://en.wikipe...|  8|[a, 3d, printing,...|(1000,[0,1,2,3,4,...|\n",
            "|3DR is an America...|     1|  1|     0|  0|     0|https://en.wikipe...|  9|[3dr, is, an, ame...|(1000,[0,1,2,3,4,...|\n",
            "|3D Systems, headq...|     1|  0|     0|  0|     0|https://en.wikipe...| 10|[3d, systems,, he...|(1000,[0,1,2,3,4,...|\n",
            "|3Delight, is 3D c...|     0|  0|     0|  0|     1|https://en.wikipe...| 11|[3delight,, is, 3...|(1000,[0,1,2,3,4,...|\n",
            "|3DVIA is a brand ...|     1|  1|     0|  0|     0|https://en.wikipe...| 12|[3dvia, is, a, br...|(1000,[0,1,2,3,4,...|\n",
            "|3i Infotech Ltd e...|     1|  0|     0|  0|     0|https://en.wikipe...| 13|[3i, infotech, lt...|(1000,[0,1,2,3,4,...|\n",
            "|3logy is the Pino...|     1|  0|     0|  0|     0|https://en.wikipe...| 14|[3logy, is, the, ...|(1000,[0,1,2,3,4,...|\n",
            "|The 4 Hour Chef T...|     1|  0|     0|  1|     0|https://en.wikipe...| 15|[the, 4, hour, ch...|(1000,[0,1,2,3,4,...|\n",
            "|4Children was a c...|     1|  1|     0|  0|     0|https://en.wikipe...| 16|[4children, was, ...|(1000,[0,1,2,3,4,...|\n",
            "|If this article d...|     1|  0|     0|  0|     0|https://en.wikipe...| 17|[if, this, articl...|(1000,[0,1,2,3,4,...|\n",
            "|4G Americas is a ...|     0|  0|     0|  1|     0|https://en.wikipe...| 18|[4g, americas, is...|(1000,[0,1,2,3,4,...|\n",
            "|4MMM identified o...|     1|  0|     0|  0|     0|https://en.wikipe...| 19|[4mmm, identified...|(1000,[0,1,2,3,4,...|\n",
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fit/Train the LSH Model"
      ],
      "metadata": {
        "id": "PXLzH5csWi81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from  pyspark.ml.feature import MinHashLSH\n",
        "\n",
        "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashValues\", numHashTables=3)\n",
        "LSHmodel = mh.fit(vectorizedDF)\n",
        "\n",
        "LSHmodel.transform(vectorizedDF).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPp6ZbQ3WcDp",
        "outputId": "73bb798e-a5d2-4163-dd04-9477dc96268b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+--------------------+--------------------+\n",
            "|                text|advert|coi|fanpov| pr|resume|                 url| id|               words|            features|          hashValues|\n",
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+--------------------+--------------------+\n",
            "|1 Litre no Namida...|     0|  0|     1|  0|     0|https://en.wikipe...|  0|[1, litre, no, na...|(1000,[0,1,2,3,4,...|[[5595911.0], [1....|\n",
            "|1DayLater was fre...|     1|  1|     0|  0|     0|https://en.wikipe...|  1|[1daylater, was, ...|(1000,[0,1,2,3,4,...|[[9357764.0], [6....|\n",
            "|1E is a privately...|     1|  0|     0|  0|     0|https://en.wikipe...|  2|[1e, is, a, priva...|(1000,[0,1,2,3,4,...|[[5595911.0], [1....|\n",
            "|1Malaysia pronoun...|     1|  0|     0|  0|     0|https://en.wikipe...|  3|[1malaysia, prono...|(1000,[0,1,2,3,4,...|[[1041919.0], [1....|\n",
            "|The Jerusalem Bie...|     1|  0|     0|  0|     0|https://en.wikipe...|  4|[the, jerusalem, ...|(1000,[0,1,2,3,4,...|[[1.3911756E7], [...|\n",
            "|1st Round Enterpr...|     0|  0|     0|  1|     0|https://en.wikipe...|  5|[1st, round, ente...|(1000,[0,1,2,3,4,...|[[5595911.0], [4....|\n",
            "|2ergo is a provid...|     1|  0|     0|  0|     0|https://en.wikipe...|  6|[2ergo, is, a, pr...|(1000,[0,1,2,3,4,...|[[5595911.0], [3....|\n",
            "|2N Telekomunikace...|     1|  0|     0|  0|     0|https://en.wikipe...|  7|[2n, telekomunika...|(1000,[0,1,2,3,4,...|[[5595911.0], [16...|\n",
            "|A 3D printing mar...|     1|  0|     0|  0|     0|https://en.wikipe...|  8|[a, 3d, printing,...|(1000,[0,1,2,3,4,...|[[3318915.0], [1....|\n",
            "|3DR is an America...|     1|  1|     0|  0|     0|https://en.wikipe...|  9|[3dr, is, an, ame...|(1000,[0,1,2,3,4,...|[[5595911.0], [1....|\n",
            "|3D Systems, headq...|     1|  0|     0|  0|     0|https://en.wikipe...| 10|[3d, systems,, he...|(1000,[0,1,2,3,4,...|[[3318915.0], [26...|\n",
            "|3Delight, is 3D c...|     0|  0|     0|  0|     1|https://en.wikipe...| 11|[3delight,, is, 3...|(1000,[0,1,2,3,4,...|[[3318915.0], [1....|\n",
            "|3DVIA is a brand ...|     1|  1|     0|  0|     0|https://en.wikipe...| 12|[3dvia, is, a, br...|(1000,[0,1,2,3,4,...|[[5595911.0], [67...|\n",
            "|3i Infotech Ltd e...|     1|  0|     0|  0|     0|https://en.wikipe...| 13|[3i, infotech, lt...|(1000,[0,1,2,3,4,...|[[5595911.0], [8....|\n",
            "|3logy is the Pino...|     1|  0|     0|  0|     0|https://en.wikipe...| 14|[3logy, is, the, ...|(1000,[0,1,2,3,4,...|[[5595911.0], [5....|\n",
            "|The 4 Hour Chef T...|     1|  0|     0|  1|     0|https://en.wikipe...| 15|[the, 4, hour, ch...|(1000,[0,1,2,3,4,...|[[5595911.0], [5....|\n",
            "|4Children was a c...|     1|  1|     0|  0|     0|https://en.wikipe...| 16|[4children, was, ...|(1000,[0,1,2,3,4,...|[[1041919.0], [1....|\n",
            "|If this article d...|     1|  0|     0|  0|     0|https://en.wikipe...| 17|[if, this, articl...|(1000,[0,1,2,3,4,...|[[5595911.0], [26...|\n",
            "|4G Americas is a ...|     0|  0|     0|  1|     0|https://en.wikipe...| 18|[4g, americas, is...|(1000,[0,1,2,3,4,...|[[2.2227601E7], [...|\n",
            "|4MMM identified o...|     1|  0|     0|  0|     0|https://en.wikipe...| 19|[4mmm, identified...|(1000,[0,1,2,3,4,...|[[5595911.0], [1....|\n",
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Search similar pair/item"
      ],
      "metadata": {
        "id": "a_bto6d6Wtoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(cvModel.vocabulary.index(\"united\"))\n",
        "print(cvModel.vocabulary.index(\"states\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taR0Ud30WnCt",
        "outputId": "259bab13-dc23-4fed-f2a5-bb3aa0c6039e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92\n",
            "198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing searching for \"united\" \"states\"\n",
        "\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "\n",
        "# Convert the input with 3 words into 1000 size vectors\n",
        "# If the words exist in the index we will give value = 1.0, otherwise 0.0\n",
        "# Final result: key = [0, 0, ... , 1.0, ..., 1.0, 1.0, ....]\n",
        "\n",
        "key = Vectors.sparse(vocabSize, {cvModel.vocabulary.index(\"civil\"): 1.0, cvModel.vocabulary.index(\"war\"): 1.0})"
      ],
      "metadata": {
        "id": "-3ks1E3qWvYi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of neighbours\n",
        "k = 40\n",
        "\n",
        "# Search inside LSH model that we already trained\n",
        "resultDF = LSHmodel.approxNearestNeighbors(vectorizedDF, key, k)\n",
        "resultDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7AN8mLWWzz7",
        "outputId": "4b4ac183-dc0b-4ea1-c416-ca289272cda0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+---+------+---+------+---+---+-----+--------+----------+-------+\n",
            "|text|advert|coi|fanpov| pr|resume|url| id|words|features|hashValues|distCol|\n",
            "+----+------+---+------+---+------+---+---+-----+--------+----------+-------+\n",
            "+----+------+---+------+---+------+---+---+-----+--------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save Results to CSV"
      ],
      "metadata": {
        "id": "QCGGk0dSW-Ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the result into CSV\n",
        "import pandas as pd\n",
        "\n",
        "data = resultDF.toPandas()\n",
        "data.to_csv(\"result.csv\")"
      ],
      "metadata": {
        "id": "Y4cUkWh9WynA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eKndkftDXAdy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}